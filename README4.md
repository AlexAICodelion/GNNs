Related to Edge_Explainability.ipynb in code


在一个已经训练好的 GCN 上，对“单个节点的预测”，分析“哪些边（邻居关系）在起关键作用”

1、构建 PyG Explainer（解释器对象）

目标是让它能够输出：

    edge_mask：每条边的重要性
    
    edge_mask ≠ 图中“客观重要的边”
    
    edge_mask = 对“当前这个节点的预测”来说重要的边

2、选择一个要解释的测试节点，并生成解释

<img width="368" height="135" alt="image" src="https://github.com/user-attachments/assets/bf29b17e-f65f-4d1e-8b27-b5d5d11e33c6" />

3、取 node_id=1708 的 Top-k 关键边（只看与该节点相连的边）

<img width="699" height="459" alt="image" src="https://github.com/user-attachments/assets/ae3daf1c-6747-438b-b62a-737e75d5df0a" />

      incident edges count = 12
      
      表示 1708 这个节点在图里连接了多少条边-12（度数相关，注意 Cora 的边通常是有向存储/双向存储，所以数值会偏大）
      
      Top-k 列表里每一行：
      
      (s -> t)：边两端节点编号
      
      w=...：解释器认为这条边对 1708 的预测贡献的重要程度（越大越关键）
      
      y[s], y[t]：两端节点的真实类别（0-6）
      
      Cora 在 PyG 里通常把无向边存成两条有向边（u→v 和 v→u）。
      
      所以 所以 incident edges 的数量通常是 2×无向度
      
      incident edges count=12 对应大概 6 个真实邻居节点：873, 1857, 467, 2314, 2313, 1358
      
      1708 的“关键邻居”里，异类占多数（4/6）：
      
      同类（=3）：1857, 2314 （2 个）
      
      异类（≠3）：873(0), 467(0), 2313(2), 1358(2)（4 个）
      
      1708 的决策主要依赖与 (0类、2类) 邻居的高权重边，这些边可能是误判的重要结构原因：邻居越同类，越容易分类正确；邻居越混杂，越容易被拉偏


4、把“邻居标签混杂”量化成统计结果，统计关键邻居的标签构成 + 权重占比（去掉双向重复）

<img width="525" height="435" alt="image" src="https://github.com/user-attachments/assets/db49267c-2cbe-418d-a891-fd7e3221625f" />

      Label counts 是“6 个关键邻居分别属于哪些真实类别”
      
      Weight share 是“这 6 个关键邻居对解释权重的贡献占比”
      
      “unique neighbors (undirected)=6” ：把与 1708 相连的邻居去重（并把双向边视为同一个邻居）
      
      所以得到“关键邻居集合”的大小是 6

      33.33%
      
      在解释器认为最关键的邻居中，只有 1/3 和 1708 同类（真实类=3）。
      
      GCN 的信息是“聚合邻居”的，所以当关键邻居同质性低时，节点表示很容易被“拉向”其他类别特征，形成误判风险。


      权重贡献几乎是“三分天下”
      
      Weight share：
      
          label 0：38.59%
          
          label 2：23.42%
          
          label 3：38.00%
      
      这意味着：在解释器看来，支持 0 类邻居的边（38.59%）和支持 3 类邻居的边（38.00%）贡献几乎一样大。
      
      这个节点的“关键邻域证据”是冲突的——模型既接收到强烈的“你像 3 类”的邻域信号，也接收到同样强烈的“你像 0 类”的邻域信号


5、deletion Fidelity（删掉关键边，看预测掉不掉）

目标：验证解释器说“重要”的边，是否真的对模型决策有因果影响。

做法：只在图里移除 node_id 的 top-k 关键无向邻居边（双向两条一起删）

<img width="2439" height="477" alt="image" src="https://github.com/user-attachments/assets/59fd2454-6c87-48a0-aaf9-db5a951b6c09" />

          对节点 1708，把解释器给出的 top-k 关键邻居对应的边（双向）删掉，然后重新 forward
          
          以 Deletion top-6 neighbors 为例：
          
              removed_neighbors: 你删掉的 top-k 关键邻居（这里是 6 个：467, 873, 1358, 1857, 2313, 2314）
              
              edges_after: 删掉这些邻居相关的 incident edges 后，全图还剩多少边（从 10556 变成 10544，说明一共删了 12 条边，符合“无向边两条有向存储”）
              
              pred_after / conf_after: 在删边后的图上，节点 1708 的新预测类别和该类别概率
              
              conf_after_on_pred_before: 删边后，节点对“原预测类别 pred_before=1”的概率
              
              drop = conf_before - conf_after_on_pred_before: 关键指标，表示删掉解释边后，对原预测的支撑力度下降了多少
              
                    drop > 0：说明删掉的邻居确实支撑了原预测（删了就不信了）
                    
                    drop < 0：说明删掉邻居反而让它更信原预测（说明你删错了或解释不准）
          
          关键结论：删边会逐步“纠正”错误预测（说明解释边确实影响决策）
          
          BASE 是：
          
              True = 3，但 Pred_before = 1，conf_before = 0.208（本来就不自信，属于“勉强选了 1 类”）
          
                  删除少量关键邻居（top-1 / top-2）：影响很小
          
                  top-1（删 873）：drop 只有 0.0074，预测仍是 1
                  
                  说明 单独删最关键的一个邻居不足以改变决策。
                  
                  top-2（删 873,1857）：drop 0.0391，预测仍是 1
                  
                  支撑 1 类的证据在减弱，但还没到翻转。
                  
                  删除到 top-3 / top-4：预测开始翻到 2 类
                  
                  top-3（再删 467）：pred_after 变成 2
                  
                  top-4（再删 2314）：pred_after 仍为 2，且 conf_after 升到 0.372
                  
                  删掉更多“关键邻居”后，模型不再坚持 1 类，而是被另一组结构证据带向 2 类
                  
                  删除到 top-6：预测终于回到真实类 3
                  
                  top-6：pred_after = 3（纠正了），conf_after = 0.300
                  
                  同时对原错误类别 1 的概率降到 0.133（drop ≈ 0.075）
                  
          edge_mask 选出来的这批邻居边，确实在支撑模型的错误决策（pred=1）
          
          错误并不是来自某一条边，而更像是多个关键邻居共同“拉偏”
          
          当把这组“拉偏证据”剔除后，剩余邻域信息反而更支持真实类 3

6、做 Insertion Fidelity（只保留 top-k 邻居边，看还能不能复现决策）

Insertion / Sufficiency：只保留解释认为关键的 top-k 邻居边（其余边都去掉），看看模型能否仍做出相同预测/保持高置信度。

<img width="2286" height="165" alt="image" src="https://github.com/user-attachments/assets/c985e741-bbba-49d4-9030-d5b410504b50" />

    原图边数：10556
    
    只保留 node 1708 的 top-6 解释边后：只剩 6 条边
    
    原图预测：pred_before = 1，且置信度 conf_before = 0.2079（很低），说明模型在全图里对它的预测（类 1）本身就不稳定，属于“摇摆/边界点”
    
    只保留 top-6 解释边后：预测变成 pred_after = 0，置信度 conf_after = 0.4241（反而更高）
    
    同时，“对原预测类别 1 的置信度”变成 conf_after_on_pred_before = 0.1892（比 0.2079 还低一点）
    
    结论：“只剩 6 条边”会把 GCN 的信息传播机制彻底改变
    
    GCN 的两层卷积本质上是在做邻域聚合。
    
    把全图删到只剩 6 条边，相当于把 node 1708 的“信息来源”强行限制为极少数路径：
    
    原图时：1708 的表示会受到更广的图结构影响（尤其是 2-hop 传播）
    
    只留 6 条边：模型只能看到极少的邻居信息 → 表示空间会被拉到另一类（这里变成了 class 0）
    
    对于误分类节点 1708，解释器给出的 top-k 解释边并不能构成一个“sufficient explanation”（不足以单独支撑模型原预测）；模型的原决策更像是由更大范围的图上下文共同决定，且该节点本身预测置信度低、决策边界不稳定


7、对照组1709（预测正确的节点）做同样的 insertion

<img width="339" height="126" alt="image" src="https://github.com/user-attachments/assets/1a6fad0d-d282-42d1-bfda-d8533bbab140" />

8、列出 node 1709 的“关键边”（incident edges Top-K）

<img width="675" height="378" alt="image" src="https://github.com/user-attachments/assets/93c61ccd-67ae-4ae1-a6f2-9786950c5787" />

          1709 在图里一共和 10 条有向边相连
          
          Top-4 的边权重都在 0.80+，并且这些边的邻居标签全部是 2，和目标节点 1709 的标签 2 一致。
          
          只有一组邻居 1986 (label=3) 相关边（eid=6853 / 8167），但权重明显低（0.1868/0.1171），说明它对决策影响较小，甚至可能是噪声/干扰边。

9、统计关键邻居的标签分布、权重占比、同质性（homophily）

1709 的解释边主要连向哪些类别的邻居？同类邻居占比高不高？

<img width="645" height="363" alt="image" src="https://github.com/user-attachments/assets/43b83964-c0f9-45ca-b7e6-596f17c61460" />

          1709 的预测主要依赖同类邻居（label=2）
          
          1709 的无向邻居一共 5 个，其中 4 个是 label=2、1 个是 label=3
          
          所以按“邻居数量口径”的同质性（homophily）是 80%
          
          解释边的权重几乎都“投给”了同类邻居
          
          label=2 的邻居贡献了 92.60% 的解释权重
          
          label=3 的邻居只占 7.40% 的解释权重
          
          不是仅仅“邻居里同类多”，而是模型真正用来做决策的那些边（edge_mask 高的边）几乎全部指向 label=2

10、Deletion fidelity（删除关键邻居，看预测是否崩）

如果删掉解释认为最重要的邻居（或边），模型对原预测类别的置信度应该明显下降，甚至改判

<img width="2400" height="675" alt="image" src="https://github.com/user-attachments/assets/6ea3b472-1687-4a99-95ca-77c22135a4c8" />

          BASE是：
          
              node=1709，真实标签 true=2
              
              模型预测 pred_before=2（预测正确）
              
              对类别 2 的置信度 conf_before=0.8195
              
              邻居重要性排名前四个关键邻居全是同类（label=2）
              
              唯一的异类邻居（label=3）重要性明显更低（0.3040）
          
          删除 top-m 关键邻居后
              删 1 个（1739）：置信度 0.8195 → 0.7844（小降，drop=0.0351），预测仍是 2
              
              删 2 个（1738,1739）：置信度 0.7038（drop=0.1157），仍是 2
              
              删 3 个（再删 2365）：置信度直接掉到 0.3582（drop=0.4613），仍是 2，但已经很虚
              
              删 4 个（再删 1358）：预测直接翻转到 3，并且你关注的“原预测类2的置信度”只剩 0.1649（drop=0.6546）
              
              删 5 个（把异类 1986 也删了）：预测仍是 3，对原类2的置信度进一步掉到 0.1053
              
              删到 3 个同类关键邻居时，原类置信度已经“腰斩再腰斩”
              删到 4 个同类关键邻居时，模型直接从 2 改判为 3
          1709 的正确预测高度依赖于这四个同类关键邻居提供的结构证据

11、Insertion Fidelity（只保留 top-k 邻居/边，能否支撑预测）

<img width="1995" height="474" alt="image" src="https://github.com/user-attachments/assets/19f9d116-c4e2-4ec5-92ba-a1876bb7d0b9" />

          BASE是：
          
              1709 预测为 2，模型对类别 2 的概率大约 0.82
          
          留下 top-m 关键邻居后
              只留 1 个关键邻居
              只靠 一个邻居 1739（对应 2 条有向边：1709↔1739），模型仍预测 2。
              但置信度从 0.82 降到 0.60：说明证据足以维持判别方向，但信息量不足以非常确定
              
              只留 2 个关键邻居
              加入第二个关键邻居（1738）后，置信度明显上升到 0.72。
              预测仍稳定为 2，模型对 2 类的信心，随着关键同类邻居数量增加而上升，符合 GCN 的“邻域聚合强化同类信号”的机制
              
              只留 3 个关键邻居
              只保留 top-3 关键邻居（1738、1739、2365），模型对类别 2 的置信度达到 0.94，比 BASE 的 0.82 还高！！
              这些 top-3 邻居几乎就构成了模型判别 1709 的“核心证据子图”。
              完整图里还有大量其它边/邻居带来的信息，对 1709 来说要么是噪声、要么是稀释（dilute）信号。
              只保留核心证据，反而让模型更“专注”，概率更高
              
              留 4 个关键邻居
              再加一个同类关键邻居 1358，置信度到 0.96。
              这基本说明：1709 的预测几乎完全由这些同类关键邻居支撑
              
              留 5 个关键邻居（加入一个异类邻居 1986）
              加入第 5 个邻居（1986）后，置信度从 0.9602 下降到 0.9286（仍高，但明显变低）
              1986 这个异类邻居不是“支持类别 2”的证据，反而会引入一部分竞争信号，使模型对 2 的确信程度下降。
          这与 deletion 那边看到的现象一致：当删除掉关键同类邻居后，模型更容易被其它类别拉走





























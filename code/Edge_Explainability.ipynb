{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8ef960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cpu 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn.functional as F\n",
    "import torch_geometric\n",
    "print(torch.__version__, torch_geometric.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d4ea54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root=r\"C:\\Users\\Administrator\\Desktop\\GNNmodel\\data\\Planetoid\", name=\"Cora\", transform=NormalizeFeatures())\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715caefc",
   "metadata": {},
   "source": [
    "1、定义并训练 GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95c483e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(1433, 16)\n",
      "  (conv2): GCNConv(16, 7)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn. Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(1234567)\n",
    "        self.conv1 =GCNConv(dataset.num_features, hidden_channels)#1433,16\n",
    "        self.conv2 =GCNConv(hidden_channels, dataset.num_classes)#16,7\n",
    "\n",
    "    def forward(self,x, edge_index):\n",
    "        x=self.conv1(x,edge_index)#firstGCN，x---2708x1433的x，edge_index是A\n",
    "        x=x.relu()\n",
    "        x=F.dropout(x, p=0.5, training=self.training)\n",
    "        x= self.conv2(x, edge_index)#second GCN\n",
    "        return x\n",
    "    \n",
    "model = GCN(hidden_channels=16)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a1528",
   "metadata": {},
   "source": [
    "2、训练与测试函数 + 训练 200 epoch，Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1338faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:001,Loss:1.9465\n",
      "Epoch:002,Loss:1.9419\n",
      "Epoch:003,Loss:1.9363\n",
      "Epoch:004,Loss:1.9290\n",
      "Epoch:005,Loss:1.9199\n",
      "Epoch:006,Loss:1.9140\n",
      "Epoch:007,Loss:1.9079\n",
      "Epoch:008,Loss:1.8992\n",
      "Epoch:009,Loss:1.8876\n",
      "Epoch:010,Loss:1.8764\n",
      "Epoch:011,Loss:1.8656\n",
      "Epoch:012,Loss:1.8626\n",
      "Epoch:013,Loss:1.8460\n",
      "Epoch:014,Loss:1.8329\n",
      "Epoch:015,Loss:1.8225\n",
      "Epoch:016,Loss:1.8167\n",
      "Epoch:017,Loss:1.7995\n",
      "Epoch:018,Loss:1.7878\n",
      "Epoch:019,Loss:1.7716\n",
      "Epoch:020,Loss:1.7568\n",
      "Epoch:021,Loss:1.7563\n",
      "Epoch:022,Loss:1.7342\n",
      "Epoch:023,Loss:1.7092\n",
      "Epoch:024,Loss:1.7015\n",
      "Epoch:025,Loss:1.6671\n",
      "Epoch:026,Loss:1.6757\n",
      "Epoch:027,Loss:1.6609\n",
      "Epoch:028,Loss:1.6355\n",
      "Epoch:029,Loss:1.6339\n",
      "Epoch:030,Loss:1.6102\n",
      "Epoch:031,Loss:1.5964\n",
      "Epoch:032,Loss:1.5721\n",
      "Epoch:033,Loss:1.5570\n",
      "Epoch:034,Loss:1.5445\n",
      "Epoch:035,Loss:1.5093\n",
      "Epoch:036,Loss:1.4889\n",
      "Epoch:037,Loss:1.4776\n",
      "Epoch:038,Loss:1.4704\n",
      "Epoch:039,Loss:1.4263\n",
      "Epoch:040,Loss:1.3972\n",
      "Epoch:041,Loss:1.3873\n",
      "Epoch:042,Loss:1.3479\n",
      "Epoch:043,Loss:1.3485\n",
      "Epoch:044,Loss:1.3739\n",
      "Epoch:045,Loss:1.3343\n",
      "Epoch:046,Loss:1.3277\n",
      "Epoch:047,Loss:1.2770\n",
      "Epoch:048,Loss:1.2651\n",
      "Epoch:049,Loss:1.2347\n",
      "Epoch:050,Loss:1.2543\n",
      "Epoch:051,Loss:1.1622\n",
      "Epoch:052,Loss:1.1483\n",
      "Epoch:053,Loss:1.1535\n",
      "Epoch:054,Loss:1.1912\n",
      "Epoch:055,Loss:1.0880\n",
      "Epoch:056,Loss:1.1374\n",
      "Epoch:057,Loss:1.0657\n",
      "Epoch:058,Loss:1.0748\n",
      "Epoch:059,Loss:1.0654\n",
      "Epoch:060,Loss:1.0201\n",
      "Epoch:061,Loss:0.9967\n",
      "Epoch:062,Loss:1.0499\n",
      "Epoch:063,Loss:1.0116\n",
      "Epoch:064,Loss:0.9945\n",
      "Epoch:065,Loss:0.9499\n",
      "Epoch:066,Loss:0.9465\n",
      "Epoch:067,Loss:0.9633\n",
      "Epoch:068,Loss:0.9137\n",
      "Epoch:069,Loss:0.9168\n",
      "Epoch:070,Loss:0.8818\n",
      "Epoch:071,Loss:0.8984\n",
      "Epoch:072,Loss:0.8301\n",
      "Epoch:073,Loss:0.8664\n",
      "Epoch:074,Loss:0.8560\n",
      "Epoch:075,Loss:0.8457\n",
      "Epoch:076,Loss:0.8306\n",
      "Epoch:077,Loss:0.8333\n",
      "Epoch:078,Loss:0.8155\n",
      "Epoch:079,Loss:0.7878\n",
      "Epoch:080,Loss:0.8277\n",
      "Epoch:081,Loss:0.7880\n",
      "Epoch:082,Loss:0.7829\n",
      "Epoch:083,Loss:0.7829\n",
      "Epoch:084,Loss:0.7633\n",
      "Epoch:085,Loss:0.7862\n",
      "Epoch:086,Loss:0.7497\n",
      "Epoch:087,Loss:0.7760\n",
      "Epoch:088,Loss:0.7419\n",
      "Epoch:089,Loss:0.6595\n",
      "Epoch:090,Loss:0.6746\n",
      "Epoch:091,Loss:0.7432\n",
      "Epoch:092,Loss:0.6609\n",
      "Epoch:093,Loss:0.6607\n",
      "Epoch:094,Loss:0.6884\n",
      "Epoch:095,Loss:0.6596\n",
      "Epoch:096,Loss:0.6456\n",
      "Epoch:097,Loss:0.6383\n",
      "Epoch:098,Loss:0.7031\n",
      "Epoch:099,Loss:0.6437\n",
      "Epoch:100,Loss:0.6375\n",
      "Epoch:101,Loss:0.6198\n",
      "Epoch:102,Loss:0.6352\n",
      "Epoch:103,Loss:0.6056\n",
      "Epoch:104,Loss:0.6586\n",
      "Epoch:105,Loss:0.5941\n",
      "Epoch:106,Loss:0.5978\n",
      "Epoch:107,Loss:0.5879\n",
      "Epoch:108,Loss:0.6061\n",
      "Epoch:109,Loss:0.6011\n",
      "Epoch:110,Loss:0.5905\n",
      "Epoch:111,Loss:0.6000\n",
      "Epoch:112,Loss:0.5716\n",
      "Epoch:113,Loss:0.5795\n",
      "Epoch:114,Loss:0.5234\n",
      "Epoch:115,Loss:0.5814\n",
      "Epoch:116,Loss:0.5577\n",
      "Epoch:117,Loss:0.5548\n",
      "Epoch:118,Loss:0.5346\n",
      "Epoch:119,Loss:0.5264\n",
      "Epoch:120,Loss:0.5309\n",
      "Epoch:121,Loss:0.5272\n",
      "Epoch:122,Loss:0.5731\n",
      "Epoch:123,Loss:0.5770\n",
      "Epoch:124,Loss:0.5377\n",
      "Epoch:125,Loss:0.5387\n",
      "Epoch:126,Loss:0.5304\n",
      "Epoch:127,Loss:0.5059\n",
      "Epoch:128,Loss:0.5701\n",
      "Epoch:129,Loss:0.4976\n",
      "Epoch:130,Loss:0.5062\n",
      "Epoch:131,Loss:0.5339\n",
      "Epoch:132,Loss:0.4869\n",
      "Epoch:133,Loss:0.4726\n",
      "Epoch:134,Loss:0.5348\n",
      "Epoch:135,Loss:0.5008\n",
      "Epoch:136,Loss:0.5117\n",
      "Epoch:137,Loss:0.5212\n",
      "Epoch:138,Loss:0.4483\n",
      "Epoch:139,Loss:0.4452\n",
      "Epoch:140,Loss:0.4919\n",
      "Epoch:141,Loss:0.4802\n",
      "Epoch:142,Loss:0.5251\n",
      "Epoch:143,Loss:0.5059\n",
      "Epoch:144,Loss:0.4949\n",
      "Epoch:145,Loss:0.4836\n",
      "Epoch:146,Loss:0.5040\n",
      "Epoch:147,Loss:0.4689\n",
      "Epoch:148,Loss:0.4756\n",
      "Epoch:149,Loss:0.4629\n",
      "Epoch:150,Loss:0.4847\n",
      "Epoch:151,Loss:0.4449\n",
      "Epoch:152,Loss:0.4446\n",
      "Epoch:153,Loss:0.4399\n",
      "Epoch:154,Loss:0.4607\n",
      "Epoch:155,Loss:0.4422\n",
      "Epoch:156,Loss:0.4418\n",
      "Epoch:157,Loss:0.4236\n",
      "Epoch:158,Loss:0.4378\n",
      "Epoch:159,Loss:0.4547\n",
      "Epoch:160,Loss:0.4715\n",
      "Epoch:161,Loss:0.3867\n",
      "Epoch:162,Loss:0.3970\n",
      "Epoch:163,Loss:0.4133\n",
      "Epoch:164,Loss:0.4163\n",
      "Epoch:165,Loss:0.4722\n",
      "Epoch:166,Loss:0.4054\n",
      "Epoch:167,Loss:0.3936\n",
      "Epoch:168,Loss:0.4379\n",
      "Epoch:169,Loss:0.4033\n",
      "Epoch:170,Loss:0.3718\n",
      "Epoch:171,Loss:0.3857\n",
      "Epoch:172,Loss:0.3929\n",
      "Epoch:173,Loss:0.4014\n",
      "Epoch:174,Loss:0.4482\n",
      "Epoch:175,Loss:0.4185\n",
      "Epoch:176,Loss:0.4107\n",
      "Epoch:177,Loss:0.3904\n",
      "Epoch:178,Loss:0.4145\n",
      "Epoch:179,Loss:0.3785\n",
      "Epoch:180,Loss:0.3631\n",
      "Epoch:181,Loss:0.3995\n",
      "Epoch:182,Loss:0.4293\n",
      "Epoch:183,Loss:0.3609\n",
      "Epoch:184,Loss:0.3945\n",
      "Epoch:185,Loss:0.4365\n",
      "Epoch:186,Loss:0.3589\n",
      "Epoch:187,Loss:0.4201\n",
      "Epoch:188,Loss:0.3681\n",
      "Epoch:189,Loss:0.4154\n",
      "Epoch:190,Loss:0.4200\n",
      "Epoch:191,Loss:0.3768\n",
      "Epoch:192,Loss:0.4199\n",
      "Epoch:193,Loss:0.3783\n",
      "Epoch:194,Loss:0.3792\n",
      "Epoch:195,Loss:0.3427\n",
      "Epoch:196,Loss:0.3893\n",
      "Epoch:197,Loss:0.3745\n",
      "Epoch:198,Loss:0.3861\n",
      "Epoch:199,Loss:0.3978\n",
      "Epoch:200,Loss:0.3751\n",
      "Test Accuracy:0.8140\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "criterion = torch.nn.CrossEntropyLoss() # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01,weight_decay=5e-4)# Define optimizer.\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()#梯度清零\n",
    "    out= model(data.x,data.edge_index)\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()#梯度更新\n",
    "    return loss\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x,data.edge_index)\n",
    "    pred =out.argmax(dim=1)#Use the class with highest probability.\n",
    "    test_correct = pred[data.test_mask]== data.y[data.test_mask] # Check against ground-truth labels.\n",
    "    test_acc = int(test_correct.sum())/ int(data.test_mask.sum())# Derive ratio of correct predictions\n",
    "    return test_acc\n",
    "\n",
    "for epoch in range(1,201):\n",
    "    loss = train()\n",
    "    print(f'Epoch:{epoch:03d},Loss:{loss:.4f}')\n",
    "\n",
    "test_acc=test()\n",
    "print(f'Test Accuracy:{test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec38ce",
   "metadata": {},
   "source": [
    "3、安装导入“解释器”所需组件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cd944a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyG Explainability imports\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.explain.config import ModelConfig\n",
    "\n",
    "# 常用工具：子图抽取、可视化需要\n",
    "from torch_geometric.utils import k_hop_subgraph, to_networkx\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b340a1",
   "metadata": {},
   "source": [
    "4、创建 Explainer（用于边 mask）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ac01aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch_geometric.explain.explainer.Explainer object at 0x0000021FC0DE33D0>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.explain import Explainer, GNNExplainer\n",
    "from torch_geometric.explain.config import ModelConfig\n",
    "\n",
    "# 1) 让模型进入 eval（解释时通常不希望 dropout 影响结果）\n",
    "model.eval()\n",
    "\n",
    "# 2) 告诉 Explainer：这是“多分类节点分类”任务，模型输出是 logits\n",
    "model_config = ModelConfig(\n",
    "    mode='multiclass_classification',\n",
    "    task_level='node',\n",
    "    return_type='raw',   # raw = logits（还没 softmax）\n",
    ")\n",
    "\n",
    "# 3) 建立 explainer：这里用 GNNExplainer，先只做 edge mask\n",
    "explainer = Explainer(\n",
    "    model=model,\n",
    "    algorithm=GNNExplainer(epochs=200),\n",
    "    explanation_type='model',\n",
    "    node_mask_type=None,     # 先不做特征解释\n",
    "    edge_mask_type='object', # 做边解释：给每条边一个重要性权重\n",
    "    model_config=model_config,\n",
    ")\n",
    "print(explainer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667b42e5",
   "metadata": {},
   "source": [
    "5、解释一个节点（得到 edge_mask）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc07728f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id = 1708\n",
      "has edge_mask: True\n",
      "edge_mask shape: (10556,)\n",
      "edge_index shape: (2, 10556)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1) 选一个测试集节点\n",
    "node_id = int(torch.where(data.test_mask)[0][0])\n",
    "print(\"node_id =\", node_id)\n",
    "\n",
    "# 2) 生成解释（只针对这个 node_id）\n",
    "explanation = explainer(\n",
    "    x=data.x,\n",
    "    edge_index=data.edge_index,\n",
    "    index=node_id,   # 解释哪个节点的预测\n",
    ")\n",
    "\n",
    "# 3) 检查解释里是否有 edge_mask\n",
    "print(\"has edge_mask:\", explanation.edge_mask is not None)\n",
    "print(\"edge_mask shape:\", tuple(explanation.edge_mask.shape))\n",
    "print(\"edge_index shape:\", tuple(data.edge_index.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8168397b",
   "metadata": {},
   "source": [
    "6、抽取 node_id=1708 的 Top-k 关键边（只看与该节点相连的边）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f01cb06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id=1708, incident edges count = 12\n",
      "\n",
      "=== node 1708 Top-15 incident edges by edge_mask ===\n",
      "01. eid= 3471  (1708 ->  873)  w=0.9098   y[s]=3, y[t]=0\n",
      "02. eid= 7558  (1708 -> 1857)  w=0.8880   y[s]=3, y[t]=3\n",
      "03. eid= 1925  (1708 ->  467)  w=0.8830   y[s]=3, y[t]=0\n",
      "04. eid= 9511  (1708 -> 2314)  w=0.8776   y[s]=3, y[t]=3\n",
      "05. eid= 9509  (1708 -> 2313)  w=0.8742   y[s]=3, y[t]=2\n",
      "06. eid= 6845  ( 873 -> 1708)  w=0.8566   y[s]=0, y[t]=3\n",
      "07. eid= 6849  (2314 -> 1708)  w=0.8521   y[s]=3, y[t]=3\n",
      "08. eid= 6848  (2313 -> 1708)  w=0.8500   y[s]=2, y[t]=3\n",
      "09. eid= 6847  (1857 -> 1708)  w=0.8393   y[s]=3, y[t]=3\n",
      "10. eid= 6844  ( 467 -> 1708)  w=0.8236   y[s]=0, y[t]=3\n",
      "11. eid= 6846  (1358 -> 1708)  w=0.2138   y[s]=2, y[t]=3\n",
      "12. eid= 5366  (1708 -> 1358)  w=0.1066   y[s]=3, y[t]=2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 你现在解释的节点\n",
    "node_id = 1708\n",
    "top_k = 15  # 你可以改 10/20 都行\n",
    "\n",
    "# 1) 取出 edge_index 和 edge_mask\n",
    "edge_index = data.edge_index\n",
    "edge_mask = explanation.edge_mask  # 如果你变量不是 explanation，请改成 ex 或 ex_bad\n",
    "\n",
    "# 2) 找到“与 node_id 相连”的边（入边+出边都算 incident edges）\n",
    "src, dst = edge_index[0], edge_index[1]\n",
    "incident = (src == node_id) | (dst == node_id)\n",
    "incident_eids = incident.nonzero(as_tuple=False).view(-1)  # 这些是边在 edge_index 里的索引\n",
    "\n",
    "print(f\"node_id={node_id}, incident edges count = {incident_eids.numel()}\")\n",
    "\n",
    "# 3) 取出这些 incident edges 的解释权重，并排序\n",
    "incident_scores = edge_mask[incident_eids]\n",
    "sorted_idx = torch.argsort(incident_scores, descending=True)\n",
    "top_eids = incident_eids[sorted_idx[:top_k]]\n",
    "top_scores = edge_mask[top_eids]\n",
    "\n",
    "# 4) 打印 Top-k 关键边（边的两端节点 + 权重 + 两端节点真实标签）\n",
    "print(f\"\\n=== node {node_id} Top-{top_k} incident edges by edge_mask ===\")\n",
    "for rank, (eid, score) in enumerate(zip(top_eids.tolist(), top_scores.tolist()), start=1):\n",
    "    s = int(src[eid])\n",
    "    t = int(dst[eid])\n",
    "    ys = int(data.y[s])\n",
    "    yt = int(data.y[t])\n",
    "    print(f\"{rank:02d}. eid={eid:5d}  ({s:4d} -> {t:4d})  w={score:.4f}   y[s]={ys}, y[t]={yt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed0e59",
   "metadata": {},
   "source": [
    "incident edges count = ?\n",
    "表示 1708 这个节点在图里连接了多少条边（度数相关，注意 Cora 的边通常是有向存储/双向存储，所以数值会偏大）\n",
    "\n",
    "Top-k 列表里每一行：\n",
    "\n",
    "(s -> t)：边两端节点编号\n",
    "\n",
    "w=...：解释器认为这条边对 1708 的预测贡献的重要程度（越大越关键）\n",
    "\n",
    "y[s], y[t]：两端节点的真实类别（0-6）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8020aea9",
   "metadata": {},
   "source": [
    "Cora 在 PyG 里通常把无向边存成两条有向边（u→v 和 v→u）。\n",
    "所以 incident edges count=12 对应大概 6 个真实邻居节点：873, 1857, 467, 2314, 2313, 1358\n",
    "\n",
    "1708 的“关键邻居”里，异类占多数（4/6）：\n",
    "\n",
    "同类（=3）：1857, 2314 （2 个）\n",
    "异类（≠3）：873(0), 467(0), 2313(2), 1358(2)（4 个）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af8f2c0",
   "metadata": {},
   "source": [
    "1708 的决策主要依赖与 (0类、2类) 邻居的高权重边，这些边可能是误判的重要结构原因"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3615b9e4",
   "metadata": {},
   "source": [
    "7、把“邻居标签混杂”量化成统计结果，统计关键邻居的标签构成 + 权重占比（去掉双向重复）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23e303c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id=1708, true_y=3\n",
      "unique neighbors (undirected) = 6\n",
      "\n",
      "Label counts among key neighbors:\n",
      "  label 0: 2\n",
      "  label 2: 2\n",
      "  label 3: 2\n",
      "\n",
      "Weight share among key neighbors:\n",
      "  label 0: weight_sum=1.7928, share=38.59%\n",
      "  label 2: weight_sum=1.0880, share=23.42%\n",
      "  label 3: weight_sum=1.7655, share=38.00%\n",
      "\n",
      "Homophily (by count) = 2/6 = 33.33%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "node_id = 1708\n",
    "edge_index = data.edge_index\n",
    "edge_mask = explanation.edge_mask  # 需要和你当前变量一致\n",
    "\n",
    "src, dst = edge_index[0], edge_index[1]\n",
    "incident = (src == node_id) | (dst == node_id)\n",
    "incident_eids = incident.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "# 把 incident edges 映射到 “邻居节点”\n",
    "neighbors = []\n",
    "weights = []\n",
    "for eid in incident_eids.tolist():\n",
    "    s = int(src[eid]); t = int(dst[eid])\n",
    "    nb = t if s == node_id else s\n",
    "    neighbors.append(nb)\n",
    "    weights.append(float(edge_mask[eid]))\n",
    "\n",
    "# 去重（因为无向边被存成两条有向边）\n",
    "# 规则：同一个邻居 nb 可能出现两次（u->v,v->u），我们取其中更大的权重代表这条“无向连接”的重要性\n",
    "best_w = {}\n",
    "for nb, w in zip(neighbors, weights):\n",
    "    if nb not in best_w or w > best_w[nb]:\n",
    "        best_w[nb] = w\n",
    "\n",
    "true_y = int(data.y[node_id])\n",
    "label_count = defaultdict(int)\n",
    "label_weight = defaultdict(float)\n",
    "\n",
    "for nb, w in best_w.items():\n",
    "    lab = int(data.y[nb])\n",
    "    label_count[lab] += 1\n",
    "    label_weight[lab] += w\n",
    "\n",
    "total_neighbors = len(best_w)\n",
    "total_weight = sum(best_w.values())\n",
    "\n",
    "print(f\"node_id={node_id}, true_y={true_y}\")\n",
    "print(f\"unique neighbors (undirected) = {total_neighbors}\")\n",
    "print(\"\\nLabel counts among key neighbors:\")\n",
    "for lab in sorted(label_count):\n",
    "    print(f\"  label {lab}: {label_count[lab]}\")\n",
    "\n",
    "print(\"\\nWeight share among key neighbors:\")\n",
    "for lab in sorted(label_weight):\n",
    "    print(f\"  label {lab}: weight_sum={label_weight[lab]:.4f}, share={label_weight[lab]/total_weight:.2%}\")\n",
    "\n",
    "same_class = label_count.get(true_y, 0)\n",
    "print(f\"\\nHomophily (by count) = {same_class}/{total_neighbors} = {same_class/total_neighbors:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d943db",
   "metadata": {},
   "source": [
    "Label counts 是“6 个关键邻居分别属于哪些真实类别”\n",
    "\n",
    "Weight share 是“这 6 个关键邻居对解释权重的贡献占比”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57ce721",
   "metadata": {},
   "source": [
    "33.33%\n",
    "在解释器认为最关键的邻居中，只有 1/3 和 1708 同类（真实类=3）。\n",
    "GCN 的信息是“聚合邻居”的，所以当关键邻居同质性低时，节点表示很容易被“拉向”其他类别特征，形成误判风险。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348ba8fd",
   "metadata": {},
   "source": [
    "权重贡献几乎是“三分天下”\n",
    "\n",
    "Weight share：\n",
    "\n",
    "label 0：38.59%\n",
    "label 2：23.42%\n",
    "label 3：38.00%\n",
    "\n",
    "这意味着：在解释器看来，支持 0 类邻居的边（38.59%）和支持 3 类邻居的边（38.00%）贡献几乎一样大。\n",
    "这个节点的“关键邻域证据”是冲突的——模型既接收到强烈的“你像 3 类”的邻域信号，也接收到同样强烈的“你像 0 类”的邻域信号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec30da34",
   "metadata": {},
   "source": [
    "8、deletion Fidelity（删掉关键边，看预测掉不掉）\n",
    "目标：验证解释器说“重要”的边，是否真的对模型决策有因果影响。\n",
    "做法：只在图里移除 node_id 的 top-k 关键无向邻居边（双向两条一起删）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a114772f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: {'node': 1708, 'true': 3, 'pred_before': 1, 'conf_before': 0.20795901119709015}\n",
      "\n",
      "Deletion top-1 neighbors:\n",
      "{'removed_neighbors': [873], 'edges_after': 10554, 'pred_after': 1, 'conf_after': 0.20052409172058105, 'conf_after_on_pred_before': 0.20052409172058105, 'drop': 0.007434919476509094}\n",
      "\n",
      "Deletion top-2 neighbors:\n",
      "{'removed_neighbors': [873, 1857], 'edges_after': 10552, 'pred_after': 1, 'conf_after': 0.16885067522525787, 'conf_after_on_pred_before': 0.16885067522525787, 'drop': 0.039108335971832275}\n",
      "\n",
      "Deletion top-3 neighbors:\n",
      "{'removed_neighbors': [467, 873, 1857], 'edges_after': 10550, 'pred_after': 2, 'conf_after': 0.23668847978115082, 'conf_after_on_pred_before': 0.17294177412986755, 'drop': 0.035017237067222595}\n",
      "\n",
      "Deletion top-4 neighbors:\n",
      "{'removed_neighbors': [467, 873, 1857, 2314], 'edges_after': 10548, 'pred_after': 2, 'conf_after': 0.37168121337890625, 'conf_after_on_pred_before': 0.132615327835083, 'drop': 0.07534368336200714}\n",
      "\n",
      "Deletion top-6 neighbors:\n",
      "{'removed_neighbors': [467, 873, 1358, 1857, 2313, 2314], 'edges_after': 10544, 'pred_after': 3, 'conf_after': 0.3000487983226776, 'conf_after_on_pred_before': 0.13287769258022308, 'drop': 0.07508131861686707}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "node_id = 1708\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_conf(edge_index_new):\n",
    "    out = model(data.x, edge_index_new)\n",
    "    prob = F.softmax(out, dim=1)\n",
    "    pred = int(prob[node_id].argmax())\n",
    "    conf = float(prob[node_id, pred])\n",
    "    return out, prob, pred, conf\n",
    "\n",
    "# 1) baseline\n",
    "_, prob0, pred0, conf0 = predict_conf(data.edge_index)\n",
    "true_y = int(data.y[node_id])\n",
    "print(\"BASE:\", {\"node\": node_id, \"true\": true_y, \"pred_before\": pred0, \"conf_before\": conf0})\n",
    "\n",
    "# 2) 取解释器给的邻居重要性（用你上一段 best_w 的逻辑）\n",
    "src, dst = data.edge_index[0], data.edge_index[1]\n",
    "incident = (src == node_id) | (dst == node_id)\n",
    "incident_eids = incident.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "best_w = {}  # nb -> w\n",
    "for eid in incident_eids.tolist():\n",
    "    s = int(src[eid]); t = int(dst[eid])\n",
    "    nb = t if s == node_id else s\n",
    "    w = float(explanation.edge_mask[eid])\n",
    "    if nb not in best_w or w > best_w[nb]:\n",
    "        best_w[nb] = w\n",
    "\n",
    "# 3) 选择 top-k 邻居（无向）\n",
    "def remove_topk_neighbors(k):\n",
    "    top_nbs = sorted(best_w.items(), key=lambda x: x[1], reverse=True)[:k]\n",
    "    top_nbs = {nb for nb, _ in top_nbs}\n",
    "\n",
    "    # 删除所有 (node_id <-> nb) 的双向边（以及可能存在的单向）\n",
    "    keep = []\n",
    "    for i in range(data.edge_index.size(1)):\n",
    "        s = int(src[i]); t = int(dst[i])\n",
    "        if (s == node_id and t in top_nbs) or (t == node_id and s in top_nbs):\n",
    "            continue\n",
    "        keep.append(i)\n",
    "    keep = torch.tensor(keep, dtype=torch.long)\n",
    "\n",
    "    edge_index_new = data.edge_index[:, keep]\n",
    "    return edge_index_new, top_nbs, int(keep.numel())\n",
    "\n",
    "for k in [1, 2, 3, 4, 6]:\n",
    "    edge_new, top_nbs, m = remove_topk_neighbors(k)\n",
    "    _, prob1, pred1, conf1 = predict_conf(edge_new)\n",
    "    conf_on_old = float(prob1[node_id, pred0])\n",
    "    print(f\"\\nDeletion top-{k} neighbors:\")\n",
    "    print({\n",
    "        \"removed_neighbors\": sorted(list(top_nbs)),\n",
    "        \"edges_after\": m,\n",
    "        \"pred_after\": pred1,\n",
    "        \"conf_after\": conf1,\n",
    "        \"conf_after_on_pred_before\": conf_on_old,\n",
    "        \"drop\": conf0 - conf_on_old\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c403b7",
   "metadata": {},
   "source": [
    "对节点 1708，把解释器给出的 top-k 关键邻居对应的边（双向）删掉，然后重新 forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc112e",
   "metadata": {},
   "source": [
    "以 Deletion top-6 neighbors 为例：\n",
    "\n",
    "removed_neighbors: 你删掉的 top-k 关键邻居（这里是 6 个：467, 873, 1358, 1857, 2313, 2314）\n",
    "\n",
    "edges_after: 删掉这些邻居相关的 incident edges 后，全图还剩多少边（从 10556 变成 10544，说明一共删了 12 条边，符合“无向边两条有向存储”）\n",
    "\n",
    "pred_after / conf_after: 在删边后的图上，节点 1708 的新预测类别和该类别概率\n",
    "\n",
    "conf_after_on_pred_before: 删边后，节点对“原预测类别 pred_before=1”的概率\n",
    "\n",
    "drop = conf_before - conf_after_on_pred_before: 关键指标，表示删掉解释边后，对原预测的支撑力度下降了多少"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb082a6",
   "metadata": {},
   "source": [
    "关键结论：删边会逐步“纠正”错误预测（说明解释边确实影响决策）\n",
    "BASE 是：\n",
    "True = 3，但 Pred_before = 1，conf_before = 0.208（本来就不自信，属于“勉强选了 1 类”）\n",
    "\n",
    "删除少量关键邻居（top-1 / top-2）：影响很小\n",
    "\n",
    "top-1（删 873）：drop 只有 0.0074，预测仍是 1\n",
    "说明 单独删最关键的一个邻居不足以改变决策。\n",
    "\n",
    "top-2（删 873,1857）：drop 0.0391，预测仍是 1\n",
    "支撑 1 类的证据在减弱，但还没到翻转。\n",
    "\n",
    "删除到 top-3 / top-4：预测开始翻到 2 类\n",
    "\n",
    "top-3（再删 467）：pred_after 变成 2\n",
    "\n",
    "top-4（再删 2314）：pred_after 仍为 2，且 conf_after 升到 0.372\n",
    "删掉更多“关键邻居”后，模型不再坚持 1 类，而是被另一组结构证据带向 2 类\n",
    "\n",
    "删除到 top-6：预测终于回到真实类 3\n",
    "\n",
    "top-6：pred_after = 3（纠正了），conf_after = 0.300\n",
    "同时对原错误类别 1 的概率降到 0.133（drop ≈ 0.075）\n",
    "edge_mask 选出来的这批邻居边，确实在支撑模型的错误决策（pred=1）\n",
    "错误并不是来自某一条边，而更像是多个关键邻居共同“拉偏”\n",
    "当你把这组“拉偏证据”剔除后，剩余邻域信息反而更支持真实类 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec93ea",
   "metadata": {},
   "source": [
    "10、做 Insertion Fidelity（只保留 top-k 邻居边，看还能不能复现决策）\n",
    "Insertion / Sufficiency：只保留解释认为关键的 top-k 邻居边（其余边都去掉），看看模型能否仍做出相同预测/保持高置信度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eaab8323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep 6 edges for node 1708\n",
      "Original edges: 10556  ->  After insertion: 6\n",
      "\n",
      "=== Insertion result ===\n",
      "{'node': 1708, 'true': 3, 'pred_before': 1, 'conf_before': 0.20795901119709015, 'pred_after': 0, 'conf_after': 0.4241393208503723, 'conf_after_on_pred_before': 0.1891966313123703, 'kept_edges': 6}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "node_id = 1708\n",
    "top_k = 6  # 你可以之后试 2/4/8\n",
    "\n",
    "# 原始信息\n",
    "edge_index = data.edge_index\n",
    "edge_mask = explanation.edge_mask\n",
    "x = data.x\n",
    "\n",
    "# 找到与 node_id 相连的边\n",
    "src, dst = edge_index[0], edge_index[1]\n",
    "incident = (src == node_id) | (dst == node_id)\n",
    "incident_eids = incident.nonzero(as_tuple=False).view(-1)\n",
    "\n",
    "# 按 edge_mask 排序，取 Top-k\n",
    "scores = edge_mask[incident_eids]\n",
    "order = torch.argsort(scores, descending=True)\n",
    "keep_eids = incident_eids[order[:top_k]]\n",
    "\n",
    "# 构造“只保留这些边”的 edge_index\n",
    "new_edge_index = edge_index[:, keep_eids]\n",
    "\n",
    "print(f\"Keep {top_k} edges for node {node_id}\")\n",
    "print(f\"Original edges: {edge_index.size(1)}  ->  After insertion: {new_edge_index.size(1)}\")\n",
    "\n",
    "# 用这个子图重新做一次 forward\n",
    "model.eval()\n",
    "out_before = model(x, edge_index)\n",
    "out_after = model(x, new_edge_index)\n",
    "\n",
    "# 概率\n",
    "prob_before = out_before.softmax(dim=1)\n",
    "prob_after = out_after.softmax(dim=1)\n",
    "\n",
    "pred_before = prob_before[node_id].argmax().item()\n",
    "conf_before = prob_before[node_id, pred_before].item()\n",
    "\n",
    "pred_after = prob_after[node_id].argmax().item()\n",
    "conf_after = prob_after[node_id, pred_after].item()\n",
    "conf_after_on_pred_before = prob_after[node_id, pred_before].item()\n",
    "\n",
    "print(\"\\n=== Insertion result ===\")\n",
    "print({\n",
    "    \"node\": node_id,\n",
    "    \"true\": int(data.y[node_id]),\n",
    "    \"pred_before\": pred_before,\n",
    "    \"conf_before\": conf_before,\n",
    "    \"pred_after\": pred_after,\n",
    "    \"conf_after\": conf_after,\n",
    "    \"conf_after_on_pred_before\": conf_after_on_pred_before,\n",
    "    \"kept_edges\": top_k\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19499958",
   "metadata": {},
   "source": [
    "原图边数：10556\n",
    "只保留 node 1708 的 top-6 解释边后：只剩 6 条边\n",
    "原图预测：pred_before = 1，且置信度 conf_before = 0.2079（很低）\n",
    "\n",
    "只保留 top-6 解释边后：预测变成 pred_after = 0，置信度 conf_after = 0.4241（反而更高）\n",
    "\n",
    "同时，“对原预测类别 1 的置信度”变成 conf_after_on_pred_before = 0.1892（比 0.2079 还低一点）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87115b4f",
   "metadata": {},
   "source": [
    "conf_before = 0.2079 非常低，说明模型在全图里对它的预测（类 1）本身就不稳定，属于“摇摆/边界点”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73874f",
   "metadata": {},
   "source": [
    "“只剩 6 条边”会把 GCN 的信息传播机制彻底改变\n",
    "\n",
    "GCN 的两层卷积本质上是在做邻域聚合。\n",
    "你把全图删到只剩 6 条边，相当于把 node 1708 的“信息来源”强行限制为极少数路径：\n",
    "\n",
    "原图时：1708 的表示会受到更广的图结构影响（尤其是 2-hop 传播）\n",
    "\n",
    "只留 6 条边：模型只能看到极少的邻居信息 → 表示空间会被拉到另一类（这里变成了 class 0）\n",
    "对于误分类节点 1708，解释器给出的 top-k 解释边并不能构成一个“sufficient explanation”（不足以单独支撑模型原预测）；模型的原决策更像是由更大范围的图上下文共同决定，且该节点本身预测置信度低、决策边界不稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfaefaf",
   "metadata": {},
   "source": [
    "11、对照组1709（预测正确的节点）做同样的 insertion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60bd8cf",
   "metadata": {},
   "source": [
    "12、对 node 1709 生成 edge_mask（边重要性）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1d778ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id = 1709\n",
      "has edge_mask: True\n",
      "edge_mask shape: (10556,)\n",
      "edge_index shape: (2, 10556)\n"
     ]
    }
   ],
   "source": [
    "node_id = 1709\n",
    "\n",
    "model.eval()\n",
    "ex_1709 = explainer(\n",
    "    x=data.x,\n",
    "    edge_index=data.edge_index,\n",
    "    index=node_id,\n",
    ")\n",
    "\n",
    "print(\"node_id =\", node_id)\n",
    "print(\"has edge_mask:\", ex_1709.edge_mask is not None)\n",
    "print(\"edge_mask shape:\", tuple(ex_1709.edge_mask.shape))\n",
    "print(\"edge_index shape:\", tuple(data.edge_index.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1181d341",
   "metadata": {},
   "source": [
    "13、列出 node 1709 的“关键边”（incident edges Top-K）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc56b160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id=1709, incident edges count = 10\n",
      "\n",
      "=== node 1709 Top-10 incident edges by edge_mask ===\n",
      "01. eid= 6852  (1739 -> 1709)  w=0.8509   y[s]=2, y[t]=2\n",
      "02. eid= 6851  (1738 -> 1709)  w=0.8390   y[s]=2, y[t]=2\n",
      "03. eid= 6854  (2365 -> 1709)  w=0.8309   y[s]=2, y[t]=2\n",
      "04. eid= 6850  (1358 -> 1709)  w=0.7990   y[s]=2, y[t]=2\n",
      "05. eid= 6853  (1986 -> 1709)  w=0.1868   y[s]=3, y[t]=2\n",
      "06. eid= 6991  (1709 -> 1738)  w=0.1276   y[s]=2, y[t]=2\n",
      "07. eid= 9673  (1709 -> 2365)  w=0.1251   y[s]=2, y[t]=2\n",
      "08. eid= 6998  (1709 -> 1739)  w=0.1179   y[s]=2, y[t]=2\n",
      "09. eid= 8167  (1709 -> 1986)  w=0.1171   y[s]=2, y[t]=3\n",
      "10. eid= 5367  (1709 -> 1358)  w=0.1155   y[s]=2, y[t]=2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "node_id = 1709\n",
    "edge_index = data.edge_index\n",
    "edge_mask = ex_1709.edge_mask.detach().cpu()\n",
    "\n",
    "# 1) 找到所有“与 node_id 相连”的边（有向：src==node 或 dst==node）\n",
    "src, dst = edge_index[0], edge_index[1]\n",
    "incident_eids = torch.where((src == node_id) | (dst == node_id))[0]\n",
    "\n",
    "print(f\"node_id={node_id}, incident edges count = {incident_eids.numel()}\")\n",
    "\n",
    "# 2) 对这些边按 edge_mask 排序（从重要到不重要）\n",
    "scores = edge_mask[incident_eids]\n",
    "order = torch.argsort(scores, descending=True)\n",
    "top_eids = incident_eids[order]\n",
    "\n",
    "# 3) 打印 Top-15 关键边：eid, (u->v), 权重w, 以及两端真实标签\n",
    "topk = min(15, top_eids.numel())\n",
    "print(f\"\\n=== node {node_id} Top-{topk} incident edges by edge_mask ===\")\n",
    "for i in range(topk):\n",
    "    eid = int(top_eids[i])\n",
    "    u = int(src[eid])\n",
    "    v = int(dst[eid])\n",
    "    w = float(edge_mask[eid])\n",
    "    print(f\"{i+1:02d}. eid={eid:5d}  ({u:4d} -> {v:4d})  w={w:.4f}   y[s]={int(data.y[u])}, y[t]={int(data.y[v])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd573b92",
   "metadata": {},
   "source": [
    "1709 在图里一共和 10 条有向边相连\n",
    "Top-4 的边权重都在 0.80+，并且这些边的邻居标签全部是 2，和目标节点 1709 的标签 2 一致。\n",
    "\n",
    "只有一组邻居 1986 (label=3) 相关边（eid=6853 / 8167），但权重明显低（0.1868/0.1171），说明它对决策影响较小，甚至可能是噪声/干扰边。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed4ac3",
   "metadata": {},
   "source": [
    "14、统计关键邻居的标签分布、权重占比、同质性（homophily）\n",
    "1709 的解释边主要连向哪些类别的邻居？同类邻居占比高不高？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f4885ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node_id=1709, true_y=2\n",
      "unique neighbors (undirected) = 5\n",
      "\n",
      "Label counts among neighbors:\n",
      "  label 2: 4\n",
      "  label 3: 1\n",
      "\n",
      "Weight share among incident edges (by neighbor label):\n",
      "  label 2: weight_sum=3.8059, share=92.60%\n",
      "  label 3: weight_sum=0.3040, share=7.40%\n",
      "\n",
      "Homophily (by count) = 4/5 = 80.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "node_id = 1709\n",
    "edge_index = data.edge_index\n",
    "edge_mask = ex_1709.edge_mask.detach().cpu()\n",
    "\n",
    "src, dst = edge_index[0], edge_index[1]\n",
    "incident_eids = torch.where((src == node_id) | (dst == node_id))[0]\n",
    "\n",
    "# 1) “邻居”去重：无向意义下，1709连到哪些不同的邻居节点\n",
    "neighbors = []\n",
    "for eid in incident_eids.tolist():\n",
    "    u = int(src[eid]); v = int(dst[eid])\n",
    "    nb = v if u == node_id else u\n",
    "    neighbors.append(nb)\n",
    "uniq_neighbors = sorted(set(neighbors))\n",
    "\n",
    "true_y = int(data.y[node_id])\n",
    "print(f\"node_id={node_id}, true_y={true_y}\")\n",
    "print(f\"unique neighbors (undirected) = {len(uniq_neighbors)}\")\n",
    "\n",
    "# 2) 邻居标签计数\n",
    "label_counts = Counter(int(data.y[n]) for n in uniq_neighbors)\n",
    "print(\"\\nLabel counts among neighbors:\")\n",
    "for lab, cnt in sorted(label_counts.items()):\n",
    "    print(f\"  label {lab}: {cnt}\")\n",
    "\n",
    "# 3) 权重占比（把 incident edges 的 edge_mask 按“邻居标签”累加）\n",
    "weight_sum_by_label = defaultdict(float)\n",
    "total_w = 0.0\n",
    "\n",
    "for eid in incident_eids.tolist():\n",
    "    u = int(src[eid]); v = int(dst[eid])\n",
    "    nb = v if u == node_id else u\n",
    "    lab = int(data.y[nb])\n",
    "    w = float(edge_mask[eid])\n",
    "    weight_sum_by_label[lab] += w\n",
    "    total_w += w\n",
    "\n",
    "print(\"\\nWeight share among incident edges (by neighbor label):\")\n",
    "for lab, wsum in sorted(weight_sum_by_label.items()):\n",
    "    share = (wsum / total_w) if total_w > 0 else 0.0\n",
    "    print(f\"  label {lab}: weight_sum={wsum:.4f}, share={share:.2%}\")\n",
    "\n",
    "# 4) 同质性（按邻居“数量”口径）\n",
    "same = sum(1 for n in uniq_neighbors if int(data.y[n]) == true_y)\n",
    "homophily = same / len(uniq_neighbors) if len(uniq_neighbors) > 0 else 0.0\n",
    "print(f\"\\nHomophily (by count) = {same}/{len(uniq_neighbors)} = {homophily:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d7fe6b",
   "metadata": {},
   "source": [
    "1709 的预测主要依赖同类邻居（label=2）\n",
    "1709 的无向邻居一共 5 个，其中 4 个是 label=2、1 个是 label=3\n",
    "所以按“邻居数量口径”的同质性（homophily）是 80%\n",
    "\n",
    "解释边的权重几乎都“投给”了同类邻居\n",
    "\n",
    "label=2 的邻居贡献了 92.60% 的解释权重\n",
    "label=3 的邻居只占 7.40% 的解释权重\n",
    "不是仅仅“邻居里同类多”，而是模型真正用来做决策的那些边（edge_mask 高的边）几乎全部指向 label=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb7d591",
   "metadata": {},
   "source": [
    "15、Deletion fidelity（删除关键邻居，看预测是否崩）\n",
    "如果我们删掉解释认为最重要的邻居（或边），模型对原预测类别的置信度应该明显下降，甚至改判"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84ff99ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_2804\\439430815.py:9: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:836.)\n",
      "  conf = float(prob[node_id, pred])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: {'node': 1709, 'true': 2, 'pred_before': 2, 'conf_before': 0.8195142149925232}\n",
      "\n",
      "Neighbors ranked by importance (neighbor-level sum of edge_mask):\n",
      "01. nb=1739  y=2  weight_sum=0.9689\n",
      "02. nb=1738  y=2  weight_sum=0.9665\n",
      "03. nb=2365  y=2  weight_sum=0.9560\n",
      "04. nb=1358  y=2  weight_sum=0.9145\n",
      "05. nb=1986  y=3  weight_sum=0.3040\n",
      "\n",
      "Deletion top-1 neighbors:\n",
      "{'removed_neighbors': [1739], 'edges_after': 10554, 'pred_after': 2, 'conf_after': 0.7844098806381226, 'conf_after_on_pred_before': 0.7844098806381226, 'drop': 0.035104334354400635}\n",
      "\n",
      "Deletion top-2 neighbors:\n",
      "{'removed_neighbors': [1738, 1739], 'edges_after': 10552, 'pred_after': 2, 'conf_after': 0.7038491368293762, 'conf_after_on_pred_before': 0.7038491368293762, 'drop': 0.11566507816314697}\n",
      "\n",
      "Deletion top-3 neighbors:\n",
      "{'removed_neighbors': [1738, 1739, 2365], 'edges_after': 10550, 'pred_after': 2, 'conf_after': 0.3582023084163666, 'conf_after_on_pred_before': 0.3582023084163666, 'drop': 0.4613119065761566}\n",
      "\n",
      "Deletion top-4 neighbors:\n",
      "{'removed_neighbors': [1738, 1739, 2365, 1358], 'edges_after': 10548, 'pred_after': 3, 'conf_after': 0.40997686982154846, 'conf_after_on_pred_before': 0.16493351757526398, 'drop': 0.6545806974172592}\n",
      "\n",
      "Deletion top-5 neighbors:\n",
      "{'removed_neighbors': [1986, 1738, 1739, 1358, 2365], 'edges_after': 10546, 'pred_after': 3, 'conf_after': 0.2789016664028168, 'conf_after_on_pred_before': 0.10534225404262543, 'drop': 0.7141719609498978}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import copy\n",
    "\n",
    "def get_pred_and_conf(model, x, edge_index, node_id):\n",
    "    model.eval()\n",
    "    out = model(x, edge_index)\n",
    "    prob = out.softmax(dim=1)\n",
    "    pred = int(prob[node_id].argmax())\n",
    "    conf = float(prob[node_id, pred])\n",
    "    return pred, conf, prob[node_id].detach().cpu()\n",
    "\n",
    "def delete_neighbors(edge_index, node_id, remove_neighbors_set):\n",
    "    src, dst = edge_index\n",
    "    keep = torch.ones(src.size(0), dtype=torch.bool)\n",
    "\n",
    "    for i in range(src.size(0)):\n",
    "        u = int(src[i]); v = int(dst[i])\n",
    "        if u == node_id and v in remove_neighbors_set:\n",
    "            keep[i] = False\n",
    "        elif v == node_id and u in remove_neighbors_set:\n",
    "            keep[i] = False\n",
    "\n",
    "    return edge_index[:, keep], int(keep.sum())\n",
    "\n",
    "node_id = 1709\n",
    "edge_mask = ex_1709.edge_mask.detach().cpu()\n",
    "edge_index = data.edge_index\n",
    "\n",
    "# 1) 基线预测\n",
    "pred_before, conf_before, prob_before = get_pred_and_conf(model, data.x, edge_index, node_id)\n",
    "true_y = int(data.y[node_id])\n",
    "\n",
    "BASE = {\"node\": node_id, \"true\": true_y, \"pred_before\": pred_before, \"conf_before\": conf_before}\n",
    "print(\"BASE:\", BASE)\n",
    "\n",
    "# 2) 找 incident edges，并按 edge_mask 找“最重要的邻居”\n",
    "src, dst = edge_index[0], edge_index[1]\n",
    "incident = torch.where((src == node_id) | (dst == node_id))[0]\n",
    "\n",
    "# 把 edge 权重聚合到“邻居”层面：一个邻居可能有两条方向边\n",
    "nb_weight = {}\n",
    "for eid in incident.tolist():\n",
    "    u = int(src[eid]); v = int(dst[eid])\n",
    "    nb = v if u == node_id else u\n",
    "    nb_weight[nb] = nb_weight.get(nb, 0.0) + float(edge_mask[eid])\n",
    "\n",
    "# 邻居按重要性排序\n",
    "sorted_nbs = sorted(nb_weight.items(), key=lambda x: x[1], reverse=True)\n",
    "sorted_nbs = [nb for nb, w in sorted_nbs]\n",
    "\n",
    "print(\"\\nNeighbors ranked by importance (neighbor-level sum of edge_mask):\")\n",
    "for i, nb in enumerate(sorted_nbs, 1):\n",
    "    print(f\"{i:02d}. nb={nb:4d}  y={int(data.y[nb])}  weight_sum={nb_weight[nb]:.4f}\")\n",
    "\n",
    "# 3) 逐步删除 top-m 邻居\n",
    "for m in [1, 2, 3, 4, 5]:\n",
    "    remove = set(sorted_nbs[:m])\n",
    "    new_ei, edges_after = delete_neighbors(edge_index, node_id, remove)\n",
    "\n",
    "    pred_after, conf_after, prob_after = get_pred_and_conf(model, data.x, new_ei, node_id)\n",
    "\n",
    "    # 关注：原预测类别 pred_before 的置信度掉了多少\n",
    "    conf_after_on_pred_before = float(prob_after[pred_before])\n",
    "    drop = conf_before - conf_after_on_pred_before\n",
    "\n",
    "    print(f\"\\nDeletion top-{m} neighbors:\")\n",
    "    print({\n",
    "        \"removed_neighbors\": list(remove),\n",
    "        \"edges_after\": edges_after,\n",
    "        \"pred_after\": pred_after,\n",
    "        \"conf_after\": conf_after,\n",
    "        \"conf_after_on_pred_before\": conf_after_on_pred_before,\n",
    "        \"drop\": drop,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de217cb",
   "metadata": {},
   "source": [
    "BASE是：\n",
    "node=1709，真实标签 true=2\n",
    "模型预测 pred_before=2（预测正确）\n",
    "对类别 2 的置信度 conf_before=0.8195\n",
    "\n",
    "邻居重要性排名前四个关键邻居全是同类（label=2）\n",
    "唯一的异类邻居（label=3）重要性明显更低（0.3040）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fc41d0",
   "metadata": {},
   "source": [
    "删除 top-m 关键邻居后\n",
    "删 1 个（1739）：置信度 0.8195 → 0.7844（小降，drop=0.0351），预测仍是 2\n",
    "\n",
    "删 2 个（1738,1739）：置信度 0.7038（drop=0.1157），仍是 2\n",
    "\n",
    "删 3 个（再删 2365）：置信度直接掉到 0.3582（drop=0.4613），仍是 2，但已经很虚\n",
    "\n",
    "删 4 个（再删 1358）：预测直接翻转到 3，并且你关注的“原预测类2的置信度”只剩 0.1649（drop=0.6546）\n",
    "\n",
    "删 5 个（把异类 1986 也删了）：预测仍是 3，对原类2的置信度进一步掉到 0.1053\n",
    "\n",
    "删到 3 个同类关键邻居时，原类置信度已经“腰斩再腰斩”\n",
    "删到 4 个同类关键邻居时，模型直接从 2 改判为 3\n",
    "1709 的正确预测高度依赖于这四个同类关键邻居提供的结构证据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936104c",
   "metadata": {},
   "source": [
    "16、Insertion Fidelity（只保留 top-k 邻居/边，能否支撑预测）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "832a91bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BASE: {'node': 1709, 'true': 2, 'pred_before': 2, 'conf_before': 0.8195142149925232}\n",
      "\n",
      "Insertion keep top-1 neighbors:\n",
      "{'kept_neighbors': [1739], 'edges_after': 2, 'pred_after': 2, 'conf_after': 0.6025946736335754, 'conf_after_on_pred_before': 0.6025946736335754}\n",
      "\n",
      "Insertion keep top-2 neighbors:\n",
      "{'kept_neighbors': [1738, 1739], 'edges_after': 4, 'pred_after': 2, 'conf_after': 0.7197235822677612, 'conf_after_on_pred_before': 0.7197235822677612}\n",
      "\n",
      "Insertion keep top-3 neighbors:\n",
      "{'kept_neighbors': [1738, 1739, 2365], 'edges_after': 6, 'pred_after': 2, 'conf_after': 0.9425157904624939, 'conf_after_on_pred_before': 0.9425157904624939}\n",
      "\n",
      "Insertion keep top-4 neighbors:\n",
      "{'kept_neighbors': [1738, 1739, 2365, 1358], 'edges_after': 8, 'pred_after': 2, 'conf_after': 0.9601922035217285, 'conf_after_on_pred_before': 0.9601922035217285}\n",
      "\n",
      "Insertion keep top-5 neighbors:\n",
      "{'kept_neighbors': [1986, 1738, 1739, 1358, 2365], 'edges_after': 10, 'pred_after': 2, 'conf_after': 0.9286341071128845, 'conf_after_on_pred_before': 0.9286341071128845}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def keep_only_neighbors(edge_index, node_id, keep_neighbors_set):\n",
    "    src, dst = edge_index\n",
    "    keep = torch.zeros(src.size(0), dtype=torch.bool)\n",
    "\n",
    "    for i in range(src.size(0)):\n",
    "        u = int(src[i]); v = int(dst[i])\n",
    "        if u == node_id and v in keep_neighbors_set:\n",
    "            keep[i] = True\n",
    "        elif v == node_id and u in keep_neighbors_set:\n",
    "            keep[i] = True\n",
    "\n",
    "    return edge_index[:, keep], int(keep.sum())\n",
    "\n",
    "node_id = 1709\n",
    "\n",
    "# 基线\n",
    "pred_before, conf_before, prob_before = get_pred_and_conf(model, data.x, data.edge_index, node_id)\n",
    "true_y = int(data.y[node_id])\n",
    "print(\"BASE:\", {\"node\": node_id, \"true\": true_y, \"pred_before\": pred_before, \"conf_before\": conf_before})\n",
    "\n",
    "# 你的邻居排序已经有了：sorted_nbs\n",
    "# 这里按 top-k 邻居做 insertion\n",
    "for k in [1, 2, 3, 4, 5]:\n",
    "    keep_nbs = set(sorted_nbs[:k])\n",
    "    new_ei, edges_after = keep_only_neighbors(data.edge_index, node_id, keep_nbs)\n",
    "\n",
    "    pred_after, conf_after, prob_after = get_pred_and_conf(model, data.x, new_ei, node_id)\n",
    "    conf_after_on_pred_before = float(prob_after[pred_before])  # 原预测类的置信度\n",
    "\n",
    "    print(f\"\\nInsertion keep top-{k} neighbors:\")\n",
    "    print({\n",
    "        \"kept_neighbors\": list(keep_nbs),\n",
    "        \"edges_after\": edges_after,\n",
    "        \"pred_after\": pred_after,\n",
    "        \"conf_after\": conf_after,\n",
    "        \"conf_after_on_pred_before\": conf_after_on_pred_before,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e77e16",
   "metadata": {},
   "source": [
    "BASE是：\n",
    "1709 预测为 2，模型对类别 2 的概率大约 0.82\n",
    "\n",
    "只留 1 个关键邻居\n",
    "只靠 一个邻居 1739（对应 2 条有向边：1709↔1739），模型仍预测 2。\n",
    "但置信度从 0.82 降到 0.60：说明证据足以维持判别方向，但信息量不足以非常确定\n",
    "\n",
    "只留 2 个关键邻居\n",
    "加入第二个关键邻居（1738）后，置信度明显上升到 0.72。\n",
    "预测仍稳定为 2，模型对 2 类的信心，随着关键同类邻居数量增加而上升，符合 GCN 的“邻域聚合强化同类信号”的机制\n",
    "\n",
    "只留 3 个关键邻居\n",
    "只保留 top-3 关键邻居（1738、1739、2365），模型对类别 2 的置信度达到 0.94，比 BASE 的 0.82 还高！！\n",
    "这些 top-3 邻居几乎就构成了模型判别 1709 的“核心证据子图”。\n",
    "完整图里还有大量其它边/邻居带来的信息，对 1709 来说要么是噪声、要么是稀释（dilute）信号。\n",
    "只保留核心证据，反而让模型更“专注”，概率更高\n",
    "\n",
    "留 4 个关键邻居\n",
    "再加一个同类关键邻居 1358，置信度到 0.96。\n",
    "这基本说明：1709 的预测几乎完全由这些同类关键邻居支撑\n",
    "\n",
    "留 5 个关键邻居（加入一个异类邻居 1986）\n",
    "加入第 5 个邻居（1986）后，置信度从 0.9602 下降到 0.9286（仍高，但明显变低）\n",
    "1986 这个异类邻居不是“支持类别 2”的证据，反而会引入一部分竞争信号，使模型对 2 的确信程度下降。\n",
    "这与 deletion 那边看到的现象一致：当删除掉关键同类邻居后，模型更容易被其它类别拉走\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
